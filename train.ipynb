{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3091ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import re\n",
    "# from transformers import AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b698e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "special = {'additional_special_tokens': [\"<a>\", \"</a>\"]}\n",
    "tokenizer.add_special_tokens(special)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8ddbbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to follow https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e400833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {0: \"O\", 1: \"E\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7e173d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, jsonl_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        with open(jsonl_file, encoding='utf-8') as f:\n",
    "            lines = f.read()\n",
    "            self.data = json.loads(lines)\n",
    "            \n",
    "#         for datum in self.data:\n",
    "#             sent = \"\"\n",
    "#             label = []\n",
    "#             datum['text'] = \"Az <a>1926-os úszó-Európa-bajnokság</a>on 4 × 200 <a>méteres</a> gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. <a>Versenytársai Wanié</a> Rezső, Wanié András és Bárány István voltak.\"\n",
    "\n",
    "#             tmp = re.sub(\"\\<\\/a\\>([^ ])\", \"</a> \\g<1>\", datum['text'])\n",
    "#             tmp = re.sub(\"\\<a\\>\", \"<a> \", tmp)\n",
    "#             for w in tmp.split():\n",
    "#                 if '<a>' in w:\n",
    "#                     label.append('E')\n",
    "#                     if '</a>' in w:\n",
    "#                         sent += w[3:-4]\n",
    "#                     else:\n",
    "#                         sent += w[3:]\n",
    "#                 else:\n",
    "#                     if '</a>' in w:\n",
    "#                         label.append('E')\n",
    "#                         sent += w.replace('</a>', \" \")\n",
    "#                     else:\n",
    "#                         label.append('O')\n",
    "#                         sent += w\n",
    "#                 if not sent.endswith(\" \"):\n",
    "#                     sent += \" \"\n",
    "#             datum[\"text\"] = sent\n",
    "#             datum[\"label\"] = label\n",
    "        self.start_id = tokenizer(\"<a>\")['input_ids'][1]\n",
    "        self.end_id = tokenizer(\"</a>\")['input_ids'][1]\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize_and_align_labels(self, example):\n",
    "        tokenized_inputs = tokenizer(example[\"text\"], truncation=True)\n",
    "        labels = []\n",
    "        new_input_ids = []\n",
    "        new_token_type_ids = []\n",
    "        new_attn_mask = []\n",
    "        \n",
    "        index = 0\n",
    "        answer_state = False\n",
    "\n",
    "        for i, token_type, attn in zip(\n",
    "            tokenized_inputs[\"input_ids\"], \n",
    "            tokenized_inputs[\"token_type_ids\"], \n",
    "            tokenized_inputs[\"attention_mask\"]\n",
    "        ):\n",
    "            if i == self.start_id:\n",
    "                answer_state = True\n",
    "            elif i == self.end_id:\n",
    "                answer_state = False\n",
    "            else:\n",
    "                new_input_ids.append(i)\n",
    "                new_token_type_ids.append(token_type)\n",
    "                new_attn_mask.append(attn)\n",
    "                if answer_state == True:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "        assert answer_state == False # ensure no unenclosed answers\n",
    "        tokenized_inputs[\"input_ids\"] = new_input_ids\n",
    "        tokenized_inputs[\"token_type_ids\"] = new_token_type_ids\n",
    "        tokenized_inputs[\"attention_mask\"] = new_attn_mask\n",
    "        tokenized_inputs[\"label\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # probably want to do this as a preprocessing step so it's not called at every getitem?\n",
    "        tokenized = self.tokenize_and_align_labels(self.data[idx])\n",
    "        print(tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]))\n",
    "\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "985add92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dev = MyDataset(\"data/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d033f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Az', '1926', '-', 'os', 'ús', '##zó', '-', 'Európa', '-', 'bajnokság', 'on', '4', '×', '200', 'méteres', 'g', '##yor', '##s', '##ús', '##zás', 'vált', '##óban', 'ez', '##üst', '##érmes', 'lett', '.', 'Vers', '##eny', '##tár', '##sai', 'Bit', '##ske', '##y', 'Zoltán', ',', 'Wan', '##ié', 'András', 'és', 'Bár', '##ány', 'István', 'voltak', '.', 'A', 'következő', ',', '1927', '-', 'es', 'ús', '##zó', '-', 'Európa', '-', 'bajnokság', '##on', 'szintén', '4', '×', '200', 'méteres', 'g', '##yor', '##s', '##ús', '##zás', 'vált', '##óban', 'bronz', '##érmes', 'lett', '.', 'Vers', '##eny', '##tár', '##sai', 'Wan', '##ié', 'Re', '##zs', '##ő', ',', 'Wan', '##ié', 'András', 'és', 'Bár', '##ány', 'István', 'voltak', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11122, 11472, 118, 10427, 36318, 16993, 118, 33663, 118, 62815, 10135, 125, 247, 10777, 96856, 175, 26101, 10107, 25911, 34304, 34229, 53948, 13112, 61785, 88831, 14852, 119, 46744, 33189, 38454, 32912, 55568, 11275, 10157, 47717, 117, 47426, 43668, 43144, 10256, 72547, 19208, 22348, 24654, 119, 138, 30633, 117, 11442, 118, 10196, 36318, 16993, 118, 33663, 118, 62815, 10263, 36506, 125, 247, 10777, 96856, 175, 26101, 10107, 25911, 34304, 34229, 53948, 77663, 88831, 14852, 119, 46744, 33189, 38454, 32912, 47426, 43668, 20304, 47799, 12516, 117, 47426, 43668, 43144, 10256, 72547, 19208, 22348, 24654, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6f219938",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'A', 'per', '##ui', 'válogatott', 'tagja', '##ként', 'részt', 'vett', 'az', '1930', '-', 'as', 'világbajnokságon', 'illetve', 'az', '1927', '-', 'es', 'és', 'az', '1929', '-', 'es', 'Dél', '-', 'amerikai', 'bajnokság', '##on', '.', '[SEP]']\n",
      "[101, 138, 10178, 11990, 43826, 27103, 16713, 27628, 30620, 10360, 11028, 118, 10146, 100644, 21415, 10360, 11442, 118, 10196, 10256, 10360, 11336, 118, 10196, 58403, 118, 25489, 62815, 10263, 119, 102]\n",
      "31\n",
      "31\n",
      "['[CLS]', 'A', 'per', '##ui', 'válogatott', 'tagja', '##ként', 'részt', 'vett', 'az', '1930', '-', 'as', 'világbajnokságon', 'illetve', 'az', '1927', '-', 'es', 'és', 'az', '1929', '-', 'es', 'Dél', '-', 'amerikai', 'bajnokság', '##on', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokenized_input = dev[1]\n",
    "# print(tokenized_input[\"input_ids\"])\n",
    "# print(len(tokenized_input[\"input_ids\"]))\n",
    "# print(len(tokenized_input[\"label\"]))\n",
    "\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "87030853",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Az', '1926', '-', 'os', 'ús', '##zó', '-', 'Európa', '-', 'bajnokság', 'on', '4', '×', '200', 'méteres', 'g', '##yor', '##s', '##ús', '##zás', 'vált', '##óban', 'ez', '##üst', '##érmes', 'lett', '.', 'Vers', '##eny', '##tár', '##sai', 'Bit', '##ske', '##y', 'Zoltán', ',', 'Wan', '##ié', 'András', 'és', 'Bár', '##ány', 'István', 'voltak', '.', 'A', 'következő', ',', '1927', '-', 'es', 'ús', '##zó', '-', 'Európa', '-', 'bajnokság', '##on', 'szintén', '4', '×', '200', 'méteres', 'g', '##yor', '##s', '##ús', '##zás', 'vált', '##óban', 'bronz', '##érmes', 'lett', '.', 'Vers', '##eny', '##tár', '##sai', 'Wan', '##ié', 'Re', '##zs', '##ő', ',', 'Wan', '##ié', 'András', 'és', 'Bár', '##ány', 'István', 'voltak', '.', '[SEP]']\n",
      "['[CLS]', 'A', 'per', '##ui', 'válogatott', 'tagja', '##ként', 'részt', 'vett', 'az', '1930', '-', 'as', 'világbajnokságon', 'illetve', 'az', '1927', '-', 'es', 'és', 'az', '1929', '-', 'es', 'Dél', '-', 'amerikai', 'bajnokság', '##on', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   101,  11122,  11472,    118,  10427,  36318,  16993,    118,  33663,\n",
       "             118,  62815,  10135,    125,    247,  10777,  96856,    175,  26101,\n",
       "           10107,  25911,  34304,  34229,  53948,  13112,  61785,  88831,  14852,\n",
       "             119,  46744,  33189,  38454,  32912,  55568,  11275,  10157,  47717,\n",
       "             117,  47426,  43668,  43144,  10256,  72547,  19208,  22348,  24654,\n",
       "             119,    138,  30633,    117,  11442,    118,  10196,  36318,  16993,\n",
       "             118,  33663,    118,  62815,  10263,  36506,    125,    247,  10777,\n",
       "           96856,    175,  26101,  10107,  25911,  34304,  34229,  53948,  77663,\n",
       "           88831,  14852,    119,  46744,  33189,  38454,  32912,  47426,  43668,\n",
       "           20304,  47799,  12516,    117,  47426,  43668,  43144,  10256,  72547,\n",
       "           19208,  22348,  24654,    119,    102],\n",
       "         [   101,    138,  10178,  11990,  43826,  27103,  16713,  27628,  30620,\n",
       "           10360,  11028,    118,  10146, 100644,  21415,  10360,  11442,    118,\n",
       "           10196,  10256,  10360,  11336,    118,  10196,  58403,    118,  25489,\n",
       "           62815,  10263,    119,    102,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'label': tensor([[   0,    0,    1,    1,    1,    1,    1,    1,    1,    1,    1,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    1,    1,    1,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator([dev[0], dev[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40916e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az <a>1926-os úszó-Európa-bajnokság</a>on 4 × 200 méteres gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. Versenytársai Wanié Rezső, Wanié András és Bárány István voltak.\n",
      "3\n",
      "39\n",
      "1926-os úszó-Európa-bajnokság\n",
      "Az 1926-os úszó-Európa-bajnokság on 4 × 200 méteres gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. Versenytársai Wanié Rezső, Wanié András és Bárány István voltak.\n"
     ]
    }
   ],
   "source": [
    "# t =  dev.data[0][\"text\"]\n",
    "# print(t)\n",
    "# prev_ind = 0\n",
    "# newtext = \"\"\n",
    "# for i in re.finditer('\\<a\\>(.*)\\<\\/a\\>', dev.data[0][\"text\"]):\n",
    "#     print(i.start())\n",
    "#     print(i.end())\n",
    "#     print(i.group(1))\n",
    "#     newtext += t[prev_ind:i.start()] + i.group(1) + \" \"\n",
    "# newtext += t[i.end():]\n",
    "# print(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44769428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 10175, 118, 10196, 36318, 16993, 118, 33663, 118, 62815, 102, 154, 68073, 10929, 74178, 11305, 102], [101, 154, 68073, 10929, 74178, 11305, 102, 154, 68073, 10929, 74178, 11305, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item, lang, title, inlink_title, context, para = dev.data.iloc[0]\n",
    "\n",
    "tokenizer([title, item], [item, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb3d4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az <a>1926-os úszó-Európa-bajnokság</a> on 4 × 200 <a>méteres</a> gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. <a>Versenytársai Wanié</a> Rezső, Wanié András és Bárány István voltak.\n",
      "43\n",
      "Az 1926-os úszó-Európa-bajnokság on 4 × 200 méteres gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. Versenytársai Wanié Rezső, Wanié András és Bárány István voltak. \n",
      "43\n",
      "['O', 'E', 'E', 'O', 'O', 'O', 'O', 'E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'E', 'E', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test = \"Az <a>1926-os úszó-Európa-bajnokság</a>on 4 × 200 <a>méteres</a> gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. <a>Versenytársai Wanié</a> Rezső, Wanié András és Bárány István voltak.\"\n",
    "sent = \"\"\n",
    "label = []\n",
    "test = re.sub(\"\\<\\/a\\>([^ ])\", \"</a> \\g<1>\", test)\n",
    "for w in test.split():\n",
    "    if '<a>' in w:\n",
    "        label.append('E')\n",
    "        if '</a>' in w:\n",
    "            sent += w[3:-4]\n",
    "        else:\n",
    "            sent += w[3:]\n",
    "    else:\n",
    "        if w.endswith(\"</a>\"):\n",
    "            label.append('E')\n",
    "            sent += w[:-4]\n",
    "        else:\n",
    "            label.append('O')\n",
    "            sent += w\n",
    "    if not sent.endswith(\" \"):\n",
    "        sent += \" \"\n",
    "\n",
    "print(test)\n",
    "print(len(test.split()))\n",
    "print(sent)\n",
    "print(len(sent.split()))\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04a29e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'hu',\n",
       " 'text': 'Az 1926-os úszó-Európa-bajnokság on 4 × 200 méteres gyorsúszás váltóban ezüstérmes lett. Versenytársai Bitskey Zoltán, Wanié András és Bárány István voltak. A következő, 1927-es úszó-Európa-bajnokságon szintén 4 × 200 méteres gyorsúszás váltóban bronzérmes lett. Versenytársai Wanié Rezső, Wanié András és Bárány István voltak. ',\n",
       " 'label': ['O',\n",
       "  'E',\n",
       "  'E',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b69de78",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3865171293.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [10]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def preprocess(item):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mnlp]",
   "language": "python",
   "name": "conda-env-mnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
